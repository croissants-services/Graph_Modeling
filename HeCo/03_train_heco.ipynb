{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afde5911-0c18-4bc8-a309-6e85f6510b84",
   "metadata": {},
   "source": [
    "설명 & 커스터마이즈 포인트\n",
    "\n",
    "Schema view: P->A, P->C로부터 평균 집계 후, 타입 어텐션으로 결합 → z_sc. (원 논문의 “자기 자신 mask” 개념 반영: paper 자기 피처는 쓰지 않고 이웃만 사용)\n",
    "\n",
    "Meta-path view: P–A–P과 P–C–P 2-hop 논문 이웃을 각각 평균 집계 후 시맨틱 어텐션으로 결합 → z_mp.\n",
    "\n",
    "Projection head: 두 view 공용(가중치 공유) MLP.\n",
    "\n",
    "Loss: 배치 내 멀티-포지티브 InfoNCE. 각 쿼리 i의 포지티브는 메타패스 이웃(PAP, PCP의 union). POS_NUM_CAP로 포지티브 상한을 둘 수 있음.\n",
    "\n",
    "Downstream: 보통 semantic 또는 fused 임베딩((z_sc+z_mp)/2)을 추천에 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "836c6a03-95f0-4e3f-b43a-290359f43f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ART_DIR: /root/heco/artifacts\n",
      "OUT_DIR: /root/heco/results\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 1] 설정 / 경로 / 하이퍼파라미터\n",
    "# =========================\n",
    "from pathlib import Path\n",
    "\n",
    "# 디렉토리\n",
    "BASE_DIR = Path(\"/root/heco\")\n",
    "ART_DIR  = BASE_DIR / \"artifacts\"\n",
    "OUT_DIR  = BASE_DIR / \"results\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 학습 하이퍼파라미터\n",
    "SEED         = 42\n",
    "EPOCHS       = 80\n",
    "BATCH_SIZE   = 512\n",
    "LR           = 0.005\n",
    "WD           = 0.0001\n",
    "EMBED_DIM    = 128      # encoder 출력 차원\n",
    "PROJ_DIM     = 128      # projection head 출력 차원\n",
    "TAU          = 0.6      # temperature\n",
    "LAMBDA       = 0.5      # L = lambda * Lsc + (1-lambda)*Lmp\n",
    "DROPOUT      = 0.3 \n",
    "POS_NUM_CAP  = 20       # (선택) 각 노드의 positive 상한 (배치 내 교집합 기준)\n",
    "\n",
    "# 메타패스 사용 (02에서 만든 것)\n",
    "USE_PAP = True\n",
    "USE_PCP = True\n",
    "\n",
    "print(\"ART_DIR:\", ART_DIR)\n",
    "print(\"OUT_DIR:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "670520b7-b6e6-4b13-ab73-41647aa9f432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 2] 라이브러리 & 시드\n",
    "# =========================\n",
    "import json, math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c89cf2-d07d-4aed-8423-08de589d8b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xp: (5000, 832) Xa: (32161, 32161) Xc: (6901, 768)\n",
      "P->A CSR: (5001,) (78212,)\n",
      "P->C CSR: (5001,) (164493,)\n",
      "PAP CSR  : (5001,) (95466,)\n",
      "PCP CSR  : (5001,) (24995000,)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 3] 아티팩트 로드 (피처/엣지/메타패스)\n",
    "# =========================\n",
    "def load_npz_E(path):\n",
    "    with np.load(path) as data:\n",
    "        if \"E\" in data: return data[\"E\"]\n",
    "        raise ValueError(f\"{path} has no 'E' key.\")\n",
    "\n",
    "def load_npz_csr(path):\n",
    "    with np.load(path) as data:\n",
    "        return data[\"indptr\"], data[\"indices\"]\n",
    "\n",
    "# meta.json\n",
    "with open(ART_DIR/\"meta.json\",\"r\") as f:\n",
    "    meta = json.load(f)\n",
    "num_papers   = meta[\"num_papers\"]\n",
    "num_authors  = meta[\"num_authors\"]\n",
    "num_concepts = meta[\"num_concepts\"]\n",
    "\n",
    "# Features\n",
    "Xp = np.load(ART_DIR/\"features_papers.npz\")[\"X\"].astype(np.float32)     # (N_papers, Dp)\n",
    "Xa = np.load(ART_DIR/\"features_authors.npz\")[\"X\"].astype(np.float32)    # (N_authors, Da)\n",
    "Xc = np.load(ART_DIR/\"features_concepts.npz\")[\"X\"].astype(np.float32)   # (N_concepts, Dc)\n",
    "\n",
    "# Edges (schema용 단일 hop)\n",
    "E_PA = load_npz_E(ART_DIR/\"edges_PA.npz\")   # paper -> author\n",
    "E_PC = load_npz_E(ART_DIR/\"edges_PC.npz\")   # paper -> concept\n",
    "\n",
    "# Build CSR adjacency: P->A, P->C\n",
    "def build_csr(num_src, edges):\n",
    "    src = edges[:,0]\n",
    "    dst = edges[:,1]\n",
    "    order = np.argsort(src, kind=\"mergesort\")\n",
    "    src = src[order]; dst = dst[order]\n",
    "    counts = np.bincount(src, minlength=num_src)\n",
    "    indptr = np.zeros(num_src+1, dtype=np.int64)\n",
    "    indptr[1:] = np.cumsum(counts)\n",
    "    indices = dst.astype(np.int64, copy=False)\n",
    "    return indptr, indices\n",
    "\n",
    "PA_indptr, PA_indices = build_csr(num_papers, E_PA)  # P->A\n",
    "PC_indptr, PC_indices = build_csr(num_papers, E_PC)  # P->C\n",
    "\n",
    "# Meta-path CSR\n",
    "PAP_indptr, PAP_indices = (np.array([0]*(num_papers+1)), np.array([], dtype=np.int64))\n",
    "PCP_indptr, PCP_indices = (np.array([0]*(num_papers+1)), np.array([], dtype=np.int64))\n",
    "if USE_PAP:\n",
    "    PAP_indptr, PAP_indices = load_npz_csr(ART_DIR/\"metapath_PAP.npz\")\n",
    "if USE_PCP:\n",
    "    PCP_indptr, PCP_indices = load_npz_csr(ART_DIR/\"metapath_PCP.npz\")\n",
    "\n",
    "print(\"Xp:\", Xp.shape, \"Xa:\", Xa.shape, \"Xc:\", Xc.shape)\n",
    "print(\"P->A CSR:\", PA_indptr.shape, PA_indices.shape)\n",
    "print(\"P->C CSR:\", PC_indptr.shape, PC_indices.shape)\n",
    "print(\"PAP CSR  :\", PAP_indptr.shape, PAP_indices.shape)\n",
    "print(\"PCP CSR  :\", PCP_indptr.shape, PCP_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d103160-38d2-4cc6-bb38-4800f57cd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# [Cell 4] PyTorch 텐서 준비 & 배치 유틸\n",
    "# =========================\n",
    "# features\n",
    "Xp_t = torch.from_numpy(Xp).to(device)  # papers\n",
    "Xa_t = torch.from_numpy(Xa).to(device)  # authors\n",
    "Xc_t = torch.from_numpy(Xc).to(device)  # concepts\n",
    "\n",
    "# CSR to device (torch.int64)\n",
    "PA_indptr_t = torch.from_numpy(PA_indptr).to(device)\n",
    "PA_indices_t= torch.from_numpy(PA_indices).to(device)\n",
    "PC_indptr_t = torch.from_numpy(PC_indptr).to(device)\n",
    "PC_indices_t= torch.from_numpy(PC_indices).to(device)\n",
    "\n",
    "PAP_indptr_t= torch.from_numpy(PAP_indptr).to(device)\n",
    "PAP_indices_t=torch.from_numpy(PAP_indices).to(device)\n",
    "PCP_indptr_t= torch.from_numpy(PCP_indptr).to(device)\n",
    "PCP_indices_t=torch.from_numpy(PCP_indices).to(device)\n",
    "\n",
    "paper_indices_all = np.arange(num_papers, dtype=np.int64)\n",
    "\n",
    "def get_batch_indices(batch_size=BATCH_SIZE):\n",
    "    # 무작위 셔플 미니배치\n",
    "    perm = np.random.permutation(num_papers)\n",
    "    for i in range(0, num_papers, batch_size):\n",
    "        yield torch.from_numpy(perm[i:i+batch_size]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d0b17cb-1b9a-47fe-b982-6f03b5d54c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# [Cell 5] Schema / Meta-path 인코더 정의\n",
    "#   - Schema: P->A, P->C 이웃 집계 + 타입 어텐션\n",
    "#   - Meta-path: (PAP, PCP) 각각 집계 + 시맨틱 어텐션\n",
    "# =========================\n",
    "class TypeAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Linear(in_dim, 1, bias=True)\n",
    "    def forward(self, xs):  # list of [B, D]\n",
    "        # 스택 후 [B, K, D] -> [B, K, 1] -> softmax\n",
    "        X = torch.stack(xs, dim=1)              # [B, K, D]\n",
    "        score = self.w(torch.tanh(X))           # [B, K, 1]\n",
    "        alpha = torch.softmax(score, dim=1)     # [B, K, 1]\n",
    "        out = (alpha * X).sum(dim=1)            # [B, D]\n",
    "        return out, alpha.squeeze(-1)           # alpha: [B, K]\n",
    "\n",
    "class SchemaEncoder(nn.Module):\n",
    "    def __init__(self, d_paper_in, d_author_in, d_concept_in, d_out, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.pa_lin = nn.Linear(d_author_in, d_out)\n",
    "        self.pc_lin = nn.Linear(d_concept_in, d_out)\n",
    "        self.attn  = TypeAttention(d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def gather_neighbors(self, indptr_t, indices_t, idx_batch):\n",
    "        # indptr: [N+1], indices: [E], idx_batch: [B]\n",
    "        starts = indptr_t[idx_batch]          # [B]\n",
    "        ends   = indptr_t[idx_batch + 1]      # [B]\n",
    "        lists = []\n",
    "        for s,e in zip(starts.tolist(), ends.tolist()):\n",
    "            if e <= s: lists.append([])\n",
    "            else:      lists.append(indices_t[s:e].tolist())\n",
    "        return lists  # list of python lists (neighbor indices)\n",
    "\n",
    "    def agg_mean(self, X_table, lists, default_idx=None):\n",
    "        # lists: length B; each is list of idx\n",
    "        outs = []\n",
    "        for nb in lists:\n",
    "            if len(nb)==0:\n",
    "                if default_idx is None:\n",
    "                    outs.append(torch.zeros(X_table.size(1), device=X_table.device))\n",
    "                else:\n",
    "                    outs.append(X_table[default_idx])\n",
    "            else:\n",
    "                outs.append(X_table[nb].mean(dim=0))\n",
    "        return torch.stack(outs, dim=0)  # [B, D_in]\n",
    "\n",
    "    def forward(self, paper_idx_batch, PA_indptr, PA_indices, PC_indptr, PC_indices, Xa, Xc):\n",
    "        # 이웃 리스트\n",
    "        a_lists = self.gather_neighbors(PA_indptr, PA_indices, paper_idx_batch)  # P->A\n",
    "        c_lists = self.gather_neighbors(PC_indptr, PC_indices, paper_idx_batch)  # P->C\n",
    "\n",
    "        # 평균 집계 후 선형 -> dropout\n",
    "        a_agg = self.pa_lin(self.agg_mean(Xa, a_lists))  # [B, d_out]\n",
    "        c_agg = self.pc_lin(self.agg_mean(Xc, c_lists))  # [B, d_out]\n",
    "        a_agg = self.dropout(F.elu(a_agg))\n",
    "        c_agg = self.dropout(F.elu(c_agg))\n",
    "\n",
    "        # 타입 어텐션으로 결합\n",
    "        z_sc, alpha = self.attn([a_agg, c_agg])          # [B, d_out]\n",
    "        z_sc = F.normalize(z_sc, p=2, dim=-1)\n",
    "        return z_sc, alpha  # alpha: [B, 2]\n",
    "\n",
    "class SemanticEncoder(nn.Module):\n",
    "    def __init__(self, d_paper_in, d_out, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.pap_lin = nn.Linear(d_paper_in, d_out)\n",
    "        self.pcp_lin = nn.Linear(d_paper_in, d_out)\n",
    "        self.attn    = TypeAttention(d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def gather_neighbors(self, indptr_t, indices_t, idx_batch):\n",
    "        starts = indptr_t[idx_batch]\n",
    "        ends   = indptr_t[idx_batch + 1]\n",
    "        lists = []\n",
    "        for s,e in zip(starts.tolist(), ends.tolist()):\n",
    "            if e <= s: lists.append([])\n",
    "            else:      lists.append(indices_t[s:e].tolist())\n",
    "        return lists\n",
    "\n",
    "    def agg_mean(self, X_paper, lists, default_idx=None):\n",
    "        outs = []\n",
    "        for nb in lists:\n",
    "            if len(nb)==0:\n",
    "                if default_idx is None:\n",
    "                    outs.append(torch.zeros(X_paper.size(1), device=X_paper.device))\n",
    "                else:\n",
    "                    outs.append(X_paper[default_idx])\n",
    "            else:\n",
    "                outs.append(X_paper[nb].mean(dim=0))\n",
    "        return torch.stack(outs, dim=0)\n",
    "\n",
    "    def forward(self, paper_idx_batch, Xp, PAP_indptr, PAP_indices, PCP_indptr, PCP_indices):\n",
    "        views = []\n",
    "        if USE_PAP:\n",
    "            pap_lists = self.gather_neighbors(PAP_indptr, PAP_indices, paper_idx_batch)\n",
    "            pap_agg = self.pap_lin(self.agg_mean(Xp, pap_lists))\n",
    "            pap_agg = self.dropout(F.elu(pap_agg))\n",
    "            views.append(pap_agg)\n",
    "        if USE_PCP:\n",
    "            pcp_lists = self.gather_neighbors(PCP_indptr, PCP_indices, paper_idx_batch)\n",
    "            pcp_agg = self.pcp_lin(self.agg_mean(Xp, pcp_lists))\n",
    "            pcp_agg = self.dropout(F.elu(pcp_agg))\n",
    "            views.append(pcp_agg)\n",
    "\n",
    "        if len(views)==1:\n",
    "            z_mp = F.normalize(views[0], p=2, dim=-1)\n",
    "            alpha = torch.ones(z_mp.size(0), 1, device=z_mp.device)\n",
    "            return z_mp, alpha\n",
    "        else:\n",
    "            z_mp, alpha = self.attn(views)           # [B, d_out]\n",
    "            z_mp = F.normalize(z_mp, p=2, dim=-1)\n",
    "            return z_mp, alpha  # alpha: [B, K(meta-path 개수)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d5324e6-7a3b-4238-9f55-e607f18c63bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# [Cell 6] Projection Head & 대조 손실(멀티 양성)\n",
    "# =========================\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_in, d_out)\n",
    "        self.act = nn.ELU()\n",
    "        self.fc2 = nn.Linear(d_out, d_out)\n",
    "    def forward(self, z):\n",
    "        return self.fc2(self.act(self.fc1(z)))\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    # a: [B, D], b: [B, D] or [N, D]\n",
    "    a = F.normalize(a, p=2, dim=-1)\n",
    "    b = F.normalize(b, p=2, dim=-1)\n",
    "    return a @ b.t()\n",
    "\n",
    "@torch.no_grad()\n",
    "def build_pos_mask(batch_idx, PAP_indptr, PAP_indices, PCP_indptr, PCP_indices, pos_cap=POS_NUM_CAP):\n",
    "    \"\"\"\n",
    "    배치 내 (i,j)가 positive인지 bool mask 생성.\n",
    "    positive 기준: i의 메타패스(PAP,PCP) 이웃 안에 j가 포함되면 True.\n",
    "    \"\"\"\n",
    "    B = batch_idx.size(0)\n",
    "    mask = torch.zeros(B, B, dtype=torch.bool, device=batch_idx.device)\n",
    "    idx_list = batch_idx.tolist()\n",
    "    idx_pos_sets = []\n",
    "    for u in idx_list:\n",
    "        # 두 메타패스 이웃 union\n",
    "        pos_set = set()\n",
    "        # PAP\n",
    "        s = PAP_indptr[u].item(); e = PAP_indptr[u+1].item()\n",
    "        if e > s: pos_set.update(PAP_indices[s:e].tolist())\n",
    "        # PCP\n",
    "        s = PCP_indptr[u].item(); e = PCP_indptr[u+1].item()\n",
    "        if e > s: pos_set.update(PCP_indices[s:e].tolist())\n",
    "        # 상한 (옵션)\n",
    "        if (pos_cap is not None) and (len(pos_set) > pos_cap):\n",
    "            # 랜덤 서브샘플\n",
    "            chosen = np.random.choice(list(pos_set), size=pos_cap, replace=False).tolist()\n",
    "            pos_set = set(chosen)\n",
    "        idx_pos_sets.append(pos_set)\n",
    "\n",
    "    # 배치 내 교집합 매핑\n",
    "    for i, u in enumerate(idx_list):\n",
    "        pos_set = idx_pos_sets[i]\n",
    "        for j, v in enumerate(idx_list):\n",
    "            if i==j: \n",
    "                continue  # self는 제외(원하면 포함 가능)\n",
    "            if v in pos_set:\n",
    "                mask[i, j] = True\n",
    "    return mask  # [B,B] bool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5de1e3-e38e-4b1a-940b-86f4d854e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paper_in=832, author_in=32161, concept_in=768, embed=128, proj=128\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 7] 모델 초기화\n",
    "# =========================\n",
    "d_paper_in   = Xp_t.size(1)\n",
    "d_author_in  = Xa_t.size(1)\n",
    "d_concept_in = Xc_t.size(1)\n",
    "\n",
    "schema_enc = SchemaEncoder(d_paper_in, d_author_in, d_concept_in, d_out=EMBED_DIM, dropout=DROPOUT).to(device)\n",
    "sem_enc    = SemanticEncoder(d_paper_in, d_out=EMBED_DIM, dropout=DROPOUT).to(device)\n",
    "proj_head  = ProjectionHead(EMBED_DIM, PROJ_DIM).to(device)\n",
    "\n",
    "params = list(schema_enc.parameters()) + list(sem_enc.parameters()) + list(proj_head.parameters())\n",
    "opt = torch.optim.AdamW(params, lr=LR, weight_decay=WD)\n",
    "\n",
    "print(f\"paper_in={d_paper_in}, author_in={d_author_in}, concept_in={d_concept_in}, embed={EMBED_DIM}, proj={PROJ_DIM}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c615f734-67e7-4809-b966-c7085433a419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001] loss=11.7585 (best 11.7585)\n",
      "[Epoch 002] loss=12.0050 (best 11.7585)\n",
      "[Epoch 003] loss=12.0323 (best 11.7585)\n",
      "[Epoch 004] loss=11.7916 (best 11.7585)\n",
      "[Epoch 005] loss=11.7887 (best 11.7585)\n",
      "[Epoch 006] loss=11.7827 (best 11.7585)\n",
      "[Epoch 007] loss=12.0184 (best 11.7585)\n",
      "[Epoch 008] loss=11.7674 (best 11.7585)\n",
      "[Epoch 009] loss=11.7970 (best 11.7585)\n",
      "[Epoch 010] loss=11.8288 (best 11.7585)\n",
      "[Epoch 011] loss=11.7372 (best 11.7372)\n",
      "[Epoch 012] loss=11.7484 (best 11.7372)\n",
      "[Epoch 013] loss=11.7402 (best 11.7372)\n",
      "[Epoch 014] loss=11.7603 (best 11.7372)\n",
      "[Epoch 015] loss=12.2420 (best 11.7372)\n",
      "[Epoch 016] loss=12.0197 (best 11.7372)\n",
      "[Epoch 017] loss=11.9637 (best 11.7372)\n",
      "[Epoch 018] loss=11.8169 (best 11.7372)\n",
      "[Epoch 019] loss=11.9853 (best 11.7372)\n",
      "[Epoch 020] loss=11.7792 (best 11.7372)\n",
      "[Epoch 021] loss=11.9113 (best 11.7372)\n",
      "[Epoch 022] loss=11.8868 (best 11.7372)\n",
      "[Epoch 023] loss=11.8964 (best 11.7372)\n",
      "[Epoch 024] loss=11.8030 (best 11.7372)\n",
      "[Epoch 025] loss=11.7706 (best 11.7372)\n",
      "[Epoch 026] loss=11.8715 (best 11.7372)\n",
      "[Epoch 027] loss=11.8706 (best 11.7372)\n",
      "[Epoch 028] loss=11.8056 (best 11.7372)\n",
      "[Epoch 029] loss=11.7976 (best 11.7372)\n",
      "[Epoch 030] loss=12.0474 (best 11.7372)\n",
      "[Epoch 031] loss=12.0785 (best 11.7372)\n",
      "[Epoch 032] loss=11.9896 (best 11.7372)\n",
      "[Epoch 033] loss=11.7410 (best 11.7372)\n",
      "[Epoch 034] loss=11.8737 (best 11.7372)\n",
      "[Epoch 035] loss=11.8851 (best 11.7372)\n",
      "[Epoch 036] loss=11.8611 (best 11.7372)\n",
      "[Epoch 037] loss=11.7827 (best 11.7372)\n",
      "[Epoch 038] loss=11.8458 (best 11.7372)\n",
      "[Epoch 039] loss=12.0608 (best 11.7372)\n",
      "[Epoch 040] loss=11.9907 (best 11.7372)\n",
      "[Epoch 041] loss=11.8320 (best 11.7372)\n",
      "[Epoch 042] loss=11.9440 (best 11.7372)\n",
      "[Epoch 043] loss=11.8982 (best 11.7372)\n",
      "[Epoch 044] loss=11.8439 (best 11.7372)\n",
      "[Epoch 045] loss=11.7726 (best 11.7372)\n",
      "[Epoch 046] loss=11.7677 (best 11.7372)\n",
      "[Epoch 047] loss=11.7465 (best 11.7372)\n",
      "[Epoch 048] loss=11.5907 (best 11.5907)\n",
      "[Epoch 049] loss=11.7861 (best 11.5907)\n",
      "[Epoch 050] loss=11.8309 (best 11.5907)\n",
      "[Epoch 051] loss=11.7661 (best 11.5907)\n",
      "[Epoch 052] loss=11.7934 (best 11.5907)\n",
      "[Epoch 053] loss=11.9346 (best 11.5907)\n",
      "[Epoch 054] loss=11.7761 (best 11.5907)\n",
      "[Epoch 055] loss=11.9320 (best 11.5907)\n",
      "[Epoch 056] loss=11.8010 (best 11.5907)\n",
      "[Epoch 057] loss=12.0688 (best 11.5907)\n",
      "[Epoch 058] loss=12.0044 (best 11.5907)\n",
      "[Epoch 059] loss=11.8707 (best 11.5907)\n",
      "[Epoch 060] loss=11.8517 (best 11.5907)\n",
      "[Epoch 061] loss=11.7258 (best 11.5907)\n",
      "[Epoch 062] loss=11.8850 (best 11.5907)\n",
      "[Epoch 063] loss=11.8095 (best 11.5907)\n",
      "[Epoch 064] loss=11.9420 (best 11.5907)\n",
      "[Epoch 065] loss=11.9296 (best 11.5907)\n",
      "[Epoch 066] loss=11.7465 (best 11.5907)\n",
      "[Epoch 067] loss=11.8126 (best 11.5907)\n",
      "[Epoch 068] loss=11.9737 (best 11.5907)\n",
      "[Epoch 069] loss=11.8663 (best 11.5907)\n",
      "[Epoch 070] loss=11.9425 (best 11.5907)\n",
      "[Epoch 071] loss=11.9089 (best 11.5907)\n",
      "[Epoch 072] loss=11.7095 (best 11.5907)\n",
      "[Epoch 073] loss=12.0475 (best 11.5907)\n",
      "[Epoch 074] loss=11.8109 (best 11.5907)\n",
      "[Epoch 075] loss=11.9080 (best 11.5907)\n",
      "[Epoch 076] loss=11.8895 (best 11.5907)\n",
      "[Epoch 077] loss=11.8929 (best 11.5907)\n",
      "[Epoch 078] loss=11.9375 (best 11.5907)\n",
      "[Epoch 079] loss=11.6879 (best 11.5907)\n",
      "[Epoch 080] loss=11.9951 (best 11.5907)\n",
      "[Epoch 081] loss=11.8751 (best 11.5907)\n",
      "[Epoch 082] loss=11.8073 (best 11.5907)\n",
      "[Epoch 083] loss=11.8045 (best 11.5907)\n",
      "[Epoch 084] loss=12.1082 (best 11.5907)\n",
      "[Epoch 085] loss=11.8015 (best 11.5907)\n",
      "[Epoch 086] loss=11.7644 (best 11.5907)\n",
      "[Epoch 087] loss=11.6794 (best 11.5907)\n",
      "[Epoch 088] loss=11.8601 (best 11.5907)\n",
      "[Epoch 089] loss=11.6592 (best 11.5907)\n",
      "[Epoch 090] loss=11.9301 (best 11.5907)\n",
      "[Epoch 091] loss=11.8724 (best 11.5907)\n",
      "[Epoch 092] loss=11.8818 (best 11.5907)\n",
      "[Epoch 093] loss=11.9267 (best 11.5907)\n",
      "[Epoch 094] loss=11.9208 (best 11.5907)\n",
      "[Epoch 095] loss=11.9051 (best 11.5907)\n",
      "[Epoch 096] loss=11.9134 (best 11.5907)\n",
      "[Epoch 097] loss=11.7314 (best 11.5907)\n",
      "[Epoch 098] loss=11.9121 (best 11.5907)\n",
      "[Epoch 099] loss=11.8715 (best 11.5907)\n",
      "[Epoch 100] loss=11.9063 (best 11.5907)\n",
      "[Epoch 101] loss=11.9521 (best 11.5907)\n",
      "[Epoch 102] loss=11.8940 (best 11.5907)\n",
      "[Epoch 103] loss=11.8266 (best 11.5907)\n",
      "[Epoch 104] loss=11.8573 (best 11.5907)\n",
      "[Epoch 105] loss=11.9276 (best 11.5907)\n",
      "[Epoch 106] loss=11.7741 (best 11.5907)\n",
      "[Epoch 107] loss=11.9284 (best 11.5907)\n",
      "[Epoch 108] loss=11.9882 (best 11.5907)\n",
      "[Epoch 109] loss=11.6837 (best 11.5907)\n",
      "[Epoch 110] loss=11.8333 (best 11.5907)\n",
      "[Epoch 111] loss=11.8761 (best 11.5907)\n",
      "[Epoch 112] loss=11.8936 (best 11.5907)\n",
      "[Epoch 113] loss=11.9164 (best 11.5907)\n",
      "[Epoch 114] loss=11.9434 (best 11.5907)\n",
      "[Epoch 115] loss=11.8674 (best 11.5907)\n",
      "[Epoch 116] loss=11.8768 (best 11.5907)\n",
      "[Epoch 117] loss=11.8974 (best 11.5907)\n",
      "[Epoch 118] loss=11.8507 (best 11.5907)\n",
      "[Epoch 119] loss=11.8836 (best 11.5907)\n",
      "[Epoch 120] loss=11.8022 (best 11.5907)\n",
      "[Epoch 121] loss=11.9545 (best 11.5907)\n",
      "[Epoch 122] loss=11.7992 (best 11.5907)\n",
      "[Epoch 123] loss=11.8262 (best 11.5907)\n",
      "[Epoch 124] loss=11.8179 (best 11.5907)\n",
      "[Epoch 125] loss=11.6314 (best 11.5907)\n",
      "[Epoch 126] loss=12.0187 (best 11.5907)\n",
      "[Epoch 127] loss=12.0564 (best 11.5907)\n",
      "[Epoch 128] loss=11.8765 (best 11.5907)\n",
      "[Epoch 129] loss=11.6614 (best 11.5907)\n",
      "[Epoch 130] loss=11.9197 (best 11.5907)\n",
      "[Epoch 131] loss=11.9766 (best 11.5907)\n",
      "[Epoch 132] loss=12.0605 (best 11.5907)\n",
      "[Epoch 133] loss=11.9574 (best 11.5907)\n",
      "[Epoch 134] loss=11.6156 (best 11.5907)\n",
      "[Epoch 135] loss=11.8307 (best 11.5907)\n",
      "[Epoch 136] loss=11.9108 (best 11.5907)\n",
      "[Epoch 137] loss=11.9463 (best 11.5907)\n",
      "[Epoch 138] loss=11.9871 (best 11.5907)\n",
      "[Epoch 139] loss=11.8959 (best 11.5907)\n",
      "[Epoch 140] loss=11.8796 (best 11.5907)\n",
      "[Epoch 141] loss=11.7304 (best 11.5907)\n",
      "[Epoch 142] loss=11.8643 (best 11.5907)\n",
      "[Epoch 143] loss=11.8816 (best 11.5907)\n",
      "[Epoch 144] loss=11.6963 (best 11.5907)\n",
      "[Epoch 145] loss=11.7313 (best 11.5907)\n",
      "[Epoch 146] loss=11.6289 (best 11.5907)\n",
      "[Epoch 147] loss=11.7998 (best 11.5907)\n",
      "[Epoch 148] loss=11.8370 (best 11.5907)\n",
      "[Epoch 149] loss=11.9304 (best 11.5907)\n",
      "[Epoch 150] loss=11.8775 (best 11.5907)\n",
      "[Epoch 151] loss=11.8068 (best 11.5907)\n",
      "[Epoch 152] loss=12.0670 (best 11.5907)\n",
      "[Epoch 153] loss=11.7004 (best 11.5907)\n",
      "[Epoch 154] loss=11.7745 (best 11.5907)\n",
      "[Epoch 155] loss=12.0274 (best 11.5907)\n",
      "[Epoch 156] loss=11.8696 (best 11.5907)\n",
      "[Epoch 157] loss=11.8312 (best 11.5907)\n",
      "[Epoch 158] loss=11.9897 (best 11.5907)\n",
      "[Epoch 159] loss=11.9080 (best 11.5907)\n",
      "[Epoch 160] loss=11.9058 (best 11.5907)\n",
      "[Epoch 161] loss=11.8861 (best 11.5907)\n",
      "[Epoch 162] loss=11.8809 (best 11.5907)\n",
      "[Epoch 163] loss=12.0290 (best 11.5907)\n",
      "[Epoch 164] loss=11.7391 (best 11.5907)\n",
      "[Epoch 165] loss=11.7826 (best 11.5907)\n",
      "[Epoch 166] loss=11.8339 (best 11.5907)\n",
      "[Epoch 167] loss=11.8880 (best 11.5907)\n",
      "[Epoch 168] loss=11.8622 (best 11.5907)\n",
      "[Epoch 169] loss=11.7084 (best 11.5907)\n",
      "[Epoch 170] loss=11.7870 (best 11.5907)\n",
      "[Epoch 171] loss=11.8692 (best 11.5907)\n",
      "[Epoch 172] loss=11.8819 (best 11.5907)\n",
      "[Epoch 173] loss=11.7880 (best 11.5907)\n",
      "[Epoch 174] loss=11.8338 (best 11.5907)\n",
      "[Epoch 175] loss=11.7930 (best 11.5907)\n",
      "[Epoch 176] loss=11.8933 (best 11.5907)\n",
      "[Epoch 177] loss=11.9387 (best 11.5907)\n",
      "[Epoch 178] loss=11.9944 (best 11.5907)\n",
      "[Epoch 179] loss=11.7429 (best 11.5907)\n",
      "[Epoch 180] loss=11.8714 (best 11.5907)\n",
      "[Epoch 181] loss=11.9946 (best 11.5907)\n",
      "[Epoch 182] loss=11.7648 (best 11.5907)\n",
      "[Epoch 183] loss=11.7542 (best 11.5907)\n",
      "[Epoch 184] loss=11.8927 (best 11.5907)\n",
      "[Epoch 185] loss=11.9730 (best 11.5907)\n",
      "[Epoch 186] loss=11.9629 (best 11.5907)\n",
      "[Epoch 187] loss=11.9584 (best 11.5907)\n",
      "[Epoch 188] loss=11.8773 (best 11.5907)\n",
      "[Epoch 189] loss=11.7785 (best 11.5907)\n",
      "[Epoch 190] loss=11.8139 (best 11.5907)\n",
      "[Epoch 191] loss=12.1023 (best 11.5907)\n",
      "[Epoch 192] loss=11.8558 (best 11.5907)\n",
      "[Epoch 193] loss=12.1294 (best 11.5907)\n",
      "[Epoch 194] loss=12.0558 (best 11.5907)\n",
      "[Epoch 195] loss=11.8774 (best 11.5907)\n",
      "[Epoch 196] loss=11.8203 (best 11.5907)\n",
      "[Epoch 197] loss=11.8250 (best 11.5907)\n",
      "[Epoch 198] loss=12.0087 (best 11.5907)\n",
      "[Epoch 199] loss=11.9256 (best 11.5907)\n",
      "[Epoch 200] loss=11.8068 (best 11.5907)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 8] 학습 루프\n",
    "#  - cross-view InfoNCE (멀티-포지티브)\n",
    "# =========================\n",
    "def infoNCE_multi_pos(z_q, z_k, pos_mask, tau=TAU, eps=1e-8):\n",
    "    \"\"\"\n",
    "    z_q: [B, D], z_k: [B, D]  (batch 기준)\n",
    "    pos_mask: [B, B]  (i row의 positives: j where mask[i,j]=True)\n",
    "    \"\"\"\n",
    "    B = z_q.size(0)\n",
    "    logits = cosine_sim(z_q, z_k) / tau             # [B,B]\n",
    "    # 마스크: 자기 자신 제외\n",
    "    diag = torch.eye(B, dtype=torch.bool, device=z_q.device)\n",
    "    logits = logits.masked_fill(diag, float('-inf'))\n",
    "\n",
    "    # 안정성: row-wise log-sum-exp\n",
    "    logsumexp = torch.logsumexp(logits, dim=1)      # [B]\n",
    "\n",
    "    # 양성 쪽: 각 i에 대해 pos j들의 log-sum-exp (여러 양성 합)\n",
    "    pos_logits = logits.masked_fill(~pos_mask, float('-inf'))\n",
    "    # pos가 하나도 없으면 -inf -> exp 0 -> log 0 = -inf -> loss NaN 방지용 epsilon\n",
    "    pos_logsumexp = torch.logsumexp(pos_logits, dim=1)  # [B]\n",
    "    # 없으면 (모두 -inf) -> -inf 로 남음. eps 보정\n",
    "    pos_logsumexp = torch.where(torch.isneginf(pos_logsumexp),\n",
    "                                torch.full_like(pos_logsumexp, math.log(eps)),\n",
    "                                pos_logsumexp)\n",
    "\n",
    "    loss = -(pos_logsumexp - logsumexp).mean()\n",
    "    return loss\n",
    "\n",
    "best_loss = float('inf')\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    schema_enc.train(); sem_enc.train(); proj_head.train()\n",
    "    epoch_loss = 0.0\n",
    "    steps = 0\n",
    "    for batch_idx in get_batch_indices(BATCH_SIZE):\n",
    "        opt.zero_grad()\n",
    "        # encodings\n",
    "        z_sc, _ = schema_enc(batch_idx, PA_indptr_t, PA_indices_t, PC_indptr_t, PC_indices_t, Xa_t, Xc_t)\n",
    "        z_mp, _ = sem_enc(batch_idx, Xp_t, PAP_indptr_t, PAP_indices_t, PCP_indptr_t, PCP_indices_t)\n",
    "        # projection (공유)\n",
    "        z_sc_p = proj_head(z_sc)\n",
    "        z_mp_p = proj_head(z_mp)\n",
    "        # positives mask (배치 내)\n",
    "        pos_mask = build_pos_mask(batch_idx, PAP_indptr_t, PAP_indices_t, PCP_indptr_t, PCP_indices_t, pos_cap=POS_NUM_CAP)\n",
    "        # loss (양방향)\n",
    "        L_sc = infoNCE_multi_pos(z_sc_p, z_mp_p, pos_mask, tau=TAU)\n",
    "        L_mp = infoNCE_multi_pos(z_mp_p, z_sc_p, pos_mask, tau=TAU)\n",
    "        loss = LAMBDA * L_sc + (1.0 - LAMBDA) * L_mp\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(params, max_norm=5.0)\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        steps += 1\n",
    "\n",
    "    avg = epoch_loss / max(1, steps)\n",
    "    if avg < best_loss:\n",
    "        best_loss = avg\n",
    "    print(f\"[Epoch {epoch:03d}] loss={avg:.4f} (best {best_loss:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b20fe2a-bf10-4ddf-8ec0-d252f83693c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /root/heco/results/paper_embeddings_schema.npy /root/heco/results/paper_embeddings_semantic.npy /root/heco/results/paper_embeddings_fused.npy\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 9] 최종 임베딩 추출 & 저장\n",
    "# =========================\n",
    "schema_enc.eval(); sem_enc.eval(); proj_head.eval()\n",
    "\n",
    "# 전체를 배치로 나눠 인코딩\n",
    "def encode_all():\n",
    "    Z_sc_list, Z_mp_list = [], []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, num_papers, BATCH_SIZE):\n",
    "            batch_idx = torch.arange(i, min(i+BATCH_SIZE, num_papers), device=device)\n",
    "            z_sc, _ = schema_enc(batch_idx, PA_indptr_t, PA_indices_t, PC_indptr_t, PC_indices_t, Xa_t, Xc_t)\n",
    "            z_mp, _ = sem_enc(batch_idx, Xp_t, PAP_indptr_t, PAP_indices_t, PCP_indptr_t, PCP_indices_t)\n",
    "            Z_sc_list.append(z_sc)\n",
    "            Z_mp_list.append(z_mp)\n",
    "    Z_sc = torch.cat(Z_sc_list, dim=0)    # [N, D]\n",
    "    Z_mp = torch.cat(Z_mp_list, dim=0)    # [N, D]\n",
    "    # 보통 downstream은 meta-path view(또는 concat) 사용\n",
    "    Z_fuse = F.normalize((Z_sc + Z_mp)/2, p=2, dim=-1)\n",
    "    return Z_sc, Z_mp, Z_fuse\n",
    "\n",
    "Z_sc, Z_mp, Z = encode_all()\n",
    "\n",
    "np.save(OUT_DIR/\"paper_embeddings_schema.npy\", Z_sc.detach().cpu().numpy())\n",
    "np.save(OUT_DIR/\"paper_embeddings_semantic.npy\", Z_mp.detach().cpu().numpy())\n",
    "np.save(OUT_DIR/\"paper_embeddings_fused.npy\",    Z.detach().cpu().numpy())\n",
    "\n",
    "print(\"Saved:\",\n",
    "      OUT_DIR/\"paper_embeddings_schema.npy\",\n",
    "      OUT_DIR/\"paper_embeddings_semantic.npy\",\n",
    "      OUT_DIR/\"paper_embeddings_fused.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2aba9a73-cefa-4607-ae8b-0cd24ff0fa4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query paper_idx: 0 paper_id: https://openalex.org/W3010906965\n",
      "top-5 idx: [4517 4382 3820  112  525]\n",
      "top-5 id : ['https://openalex.org/W2936033307', 'https://openalex.org/W3177009667', 'https://openalex.org/W3048565185', 'https://openalex.org/W3020097213', 'https://openalex.org/W3128551684']\n",
      "sims     : [0.9879460334777832, 0.9831452369689941, 0.9826983213424683, 0.9824612140655518, 0.9810717105865479]\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# [Cell 10] (선택) 근접 탐색 샘플\n",
    "# =========================\n",
    "import pandas as pd\n",
    "\n",
    "paper_ids = pd.read_csv(ART_DIR/\"map_paper_id.csv\")[\"paper_id\"].tolist()\n",
    "Z_np = np.load(OUT_DIR/\"paper_embeddings_fused.npy\")\n",
    "\n",
    "def topk_neighbors(i, k=5):\n",
    "    v = Z_np[i]\n",
    "    sims = Z_np @ v\n",
    "    sims[i] = -1e9\n",
    "    idx = np.argpartition(-sims, k)[:k]\n",
    "    idx = idx[np.argsort(-sims[idx])]\n",
    "    return idx, sims[idx]\n",
    "\n",
    "test_idx = 0  # 예시\n",
    "idxs, scs = topk_neighbors(test_idx, k=5)\n",
    "print(\"query paper_idx:\", test_idx, \"paper_id:\", paper_ids[test_idx] if test_idx < len(paper_ids) else \"NA\")\n",
    "print(\"top-5 idx:\", idxs)\n",
    "print(\"top-5 id :\", [paper_ids[i] for i in idxs])\n",
    "print(\"sims     :\", [float(s) for s in scs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874fd6ec-fe13-4146-be6c-bf22f360653d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
